{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM24adddtVjfRMcsYHRnS4U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/momna763/Target-Lock/blob/main/Targetlock_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Moi7241bAkk5",
        "outputId": "eb695923-8e55-4756-be28-7b866f3834a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.36.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting chromedriver-autoinstaller\n",
            "  Downloading chromedriver_autoinstaller-0.6.4-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Collecting pymongo\n",
            "  Downloading pymongo-4.15.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (22 kB)\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: urllib3<3.0,>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
            "Collecting trio<1.0,>=0.30.0 (from selenium)\n",
            "  Downloading trio-0.31.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket<1.0,>=0.12.2 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2025.6.15 in /usr/local/lib/python3.12/dist-packages (from selenium) (2025.8.3)\n",
            "Requirement already satisfied: typing_extensions<5.0,>=4.14.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (4.15.0)\n",
            "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.12/dist-packages (from chromedriver-autoinstaller) (25.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
            "  Downloading dnspython-2.8.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from bs4) (4.13.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.30.0->selenium) (3.10)\n",
            "Collecting outcome (from trio<1.0,>=0.30.0->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.30.0->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket<1.0,>=0.12.2->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->bs4) (2.8)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium) (0.16.0)\n",
            "Downloading selenium-4.36.0-py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chromedriver_autoinstaller-0.6.4-py3-none-any.whl (7.6 kB)\n",
            "Downloading pymongo-4.15.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Downloading dnspython-2.8.0-py3-none-any.whl (331 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.31.0-py3-none-any.whl (512 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.7/512.7 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, outcome, dnspython, chromedriver-autoinstaller, trio, pymongo, bs4, trio-websocket, selenium\n",
            "Successfully installed bs4-0.0.2 chromedriver-autoinstaller-0.6.4 dnspython-2.8.0 outcome-1.3.0.post0 pymongo-4.15.3 selenium-4.36.0 trio-0.31.0 trio-websocket-0.12.2 wsproto-1.2.0\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:2 https://cli.github.com/packages stable InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,276 kB]\n",
            "Get:13 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [32.8 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,582 kB]\n",
            "Get:15 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [44.9 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,425 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,750 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,727 kB]\n",
            "Fetched 16.3 MB in 3s (4,737 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libxi6 is already the newest version (2:1.8-1build1).\n",
            "libxi6 set to manually installed.\n",
            "unzip is already the newest version (6.0-26ubuntu3.2).\n",
            "wget is already the newest version (1.21.2-2ubuntu1.1).\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.15).\n",
            "The following NEW packages will be installed:\n",
            "  gconf-service gconf-service-backend gconf2-common libdbus-glib-1-2\n",
            "  libgconf-2-4\n",
            "0 upgraded, 5 newly installed, 0 to remove and 42 not upgraded.\n",
            "Need to get 926 kB of archives.\n",
            "After this operation, 8,309 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libdbus-glib-1-2 amd64 0.112-2build1 [65.4 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 gconf2-common all 3.2.6-7ubuntu2 [698 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libgconf-2-4 amd64 3.2.6-7ubuntu2 [86.0 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 gconf-service-backend amd64 3.2.6-7ubuntu2 [59.3 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 gconf-service amd64 3.2.6-7ubuntu2 [17.4 kB]\n",
            "Fetched 926 kB in 1s (827 kB/s)\n",
            "Selecting previously unselected package libdbus-glib-1-2:amd64.\n",
            "(Reading database ... 126675 files and directories currently installed.)\n",
            "Preparing to unpack .../libdbus-glib-1-2_0.112-2build1_amd64.deb ...\n",
            "Unpacking libdbus-glib-1-2:amd64 (0.112-2build1) ...\n",
            "Selecting previously unselected package gconf2-common.\n",
            "Preparing to unpack .../gconf2-common_3.2.6-7ubuntu2_all.deb ...\n",
            "Unpacking gconf2-common (3.2.6-7ubuntu2) ...\n",
            "Selecting previously unselected package libgconf-2-4:amd64.\n",
            "Preparing to unpack .../libgconf-2-4_3.2.6-7ubuntu2_amd64.deb ...\n",
            "Unpacking libgconf-2-4:amd64 (3.2.6-7ubuntu2) ...\n",
            "Selecting previously unselected package gconf-service-backend.\n",
            "Preparing to unpack .../gconf-service-backend_3.2.6-7ubuntu2_amd64.deb ...\n",
            "Unpacking gconf-service-backend (3.2.6-7ubuntu2) ...\n",
            "Selecting previously unselected package gconf-service.\n",
            "Preparing to unpack .../gconf-service_3.2.6-7ubuntu2_amd64.deb ...\n",
            "Unpacking gconf-service (3.2.6-7ubuntu2) ...\n",
            "Setting up gconf2-common (3.2.6-7ubuntu2) ...\n",
            "\n",
            "Creating config file /etc/gconf/2/path with new version\n",
            "Setting up libdbus-glib-1-2:amd64 (0.112-2build1) ...\n",
            "Setting up libgconf-2-4:amd64 (3.2.6-7ubuntu2) ...\n",
            "Setting up gconf-service (3.2.6-7ubuntu2) ...\n",
            "Setting up gconf-service-backend (3.2.6-7ubuntu2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/bin/bash: line 1: google-chrome: command not found\n",
            "Chrome not found — will install via script\n",
            "✅ All dependencies installed successfully.\n"
          ]
        }
      ],
      "source": [
        "# =========================================\n",
        "# 📦 INSTALL DEPENDENCIES\n",
        "# =========================================\n",
        "!pip install selenium chromedriver-autoinstaller pandas pymongo bs4\n",
        "!apt-get update\n",
        "!apt-get install -y wget unzip xvfb libxi6 libgconf-2-4\n",
        "!google-chrome --version || echo \"Chrome not found — will install via script\"\n",
        "\n",
        "# Optional: confirm installations\n",
        "import pandas, selenium, pymongo\n",
        "print(\"✅ All dependencies installed successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# 🌐 CONNECT TO MONGODB\n",
        "# =========================================\n",
        "from pymongo import MongoClient\n",
        "\n",
        "# ⚠️ Replace with your actual connection string:\n",
        "MONGO_URI = \"mongodb+srv://targetlock_user:momna123@cluster0.n8autuj.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
        "\n",
        "client = MongoClient(MONGO_URI)\n",
        "db = client[\"test\"]\n",
        "\n",
        "print(\"✅ Connected to MongoDB:\", db.name)\n",
        "print(\"📦 Collections:\", db.list_collection_names())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vHofqpLArGE",
        "outputId": "d14ca75c-97db-4f22-f0b3-92241ebe6f7c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Connected to MongoDB: test\n",
            "📦 Collections: ['smartphones_clean', 'trends', 'reports', 'users', 'products', 'smartphones']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# 🧩 FIX: Install Google Chrome (for Selenium)\n",
        "# =========================================\n",
        "!wget -q -O /tmp/google-chrome-stable_current_amd64.deb https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "!apt install -y /tmp/google-chrome-stable_current_amd64.deb\n",
        "\n",
        "import chromedriver_autoinstaller\n",
        "chromedriver_autoinstaller.install()\n",
        "\n",
        "print(\"✅ Google Chrome & ChromeDriver installed successfully!\")\n",
        "!google-chrome --version\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9fL8DYNBpVo",
        "outputId": "7f7a6b3f-4fca-4141-de0a-bd2b0127a966"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Note, selecting 'google-chrome-stable' instead of '/tmp/google-chrome-stable_current_amd64.deb'\n",
            "The following additional packages will be installed:\n",
            "  libvulkan1 mesa-vulkan-drivers\n",
            "The following NEW packages will be installed:\n",
            "  google-chrome-stable libvulkan1 mesa-vulkan-drivers\n",
            "0 upgraded, 3 newly installed, 0 to remove and 42 not upgraded.\n",
            "Need to get 10.9 MB/131 MB of archives.\n",
            "After this operation, 448 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libvulkan1 amd64 1.3.204.1-2 [128 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 mesa-vulkan-drivers amd64 23.2.1-1ubuntu3.1~22.04.3 [10.7 MB]\n",
            "Get:3 /tmp/google-chrome-stable_current_amd64.deb google-chrome-stable amd64 141.0.7390.65-1 [121 MB]\n",
            "Fetched 10.9 MB in 2s (6,292 kB/s)\n",
            "Selecting previously unselected package libvulkan1:amd64.\n",
            "(Reading database ... 126838 files and directories currently installed.)\n",
            "Preparing to unpack .../libvulkan1_1.3.204.1-2_amd64.deb ...\n",
            "Unpacking libvulkan1:amd64 (1.3.204.1-2) ...\n",
            "Selecting previously unselected package google-chrome-stable.\n",
            "Preparing to unpack .../google-chrome-stable_current_amd64.deb ...\n",
            "Unpacking google-chrome-stable (141.0.7390.65-1) ...\n",
            "Selecting previously unselected package mesa-vulkan-drivers:amd64.\n",
            "Preparing to unpack .../mesa-vulkan-drivers_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\n",
            "Unpacking mesa-vulkan-drivers:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up libvulkan1:amd64 (1.3.204.1-2) ...\n",
            "Setting up mesa-vulkan-drivers:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up google-chrome-stable (141.0.7390.65-1) ...\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/google-chrome (google-chrome) in auto mode\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "✅ Google Chrome & ChromeDriver installed successfully!\n",
            "Google Chrome 141.0.7390.65 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 📦 DARAZ Multi-Category Scraper (Final Version for College Project)\n",
        "# ============================================================\n",
        "\n",
        "import requests\n",
        "from pymongo import MongoClient\n",
        "from datetime import datetime\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1️⃣ MongoDB Connection (use your existing cluster)\n",
        "# ------------------------------------------------------------\n",
        "client = MongoClient(\"mongodb+srv://targetlock_user:momna123@cluster0.n8autuj.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\")\n",
        "db = client[\"test\"]               # ✅ Using your main frontend-connected DB\n",
        "collection = db[\"products\"]       # ✅ All categories stored here\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2️⃣ Categories to scrape\n",
        "# ------------------------------------------------------------\n",
        "categories = {\n",
        "    \"smartphones\": \"https://www.daraz.pk/catalog/?q=smartphones\",\n",
        "    \"laptops\": \"https://www.daraz.pk/catalog/?q=laptops\",\n",
        "    \"tablets\": \"https://www.daraz.pk/catalog/?q=tablets\",\n",
        "    \"smartwatches\": \"https://www.daraz.pk/catalog/?q=smartwatches\"\n",
        "}\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3️⃣ Function to fetch and insert data\n",
        "# ------------------------------------------------------------\n",
        "def scrape_category(category, base_url, pages=3, limit_per_category=50):\n",
        "    all_products = []\n",
        "    print(f\"\\n🔍 Scraping category: {category}\")\n",
        "\n",
        "    for page in range(1, pages + 1):\n",
        "        url = f\"{base_url}&page={page}&ajax=true\"\n",
        "        r = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "        if r.status_code == 200:\n",
        "            data = r.json()\n",
        "            items = data.get(\"mods\", {}).get(\"listItems\", [])\n",
        "            print(f\"✅ Page {page}: {len(items)} items scraped.\")\n",
        "            for item in items:\n",
        "                product = {\n",
        "                    \"name\": item.get(\"name\"),\n",
        "                    \"price\": item.get(\"price\"),\n",
        "                    \"image\": item.get(\"image\"),\n",
        "                    \"productUrl\": item.get(\"productUrl\"),\n",
        "                    \"ratingScore\": item.get(\"ratingScore\"),\n",
        "                    \"review\": item.get(\"review\"),\n",
        "                    \"sellerName\": item.get(\"sellerName\"),\n",
        "                    \"source\": \"daraz.pk\",\n",
        "                    \"category\": category,\n",
        "                    \"scrapedAt\": datetime.utcnow()\n",
        "                }\n",
        "                all_products.append(product)\n",
        "\n",
        "    if not all_products:\n",
        "        print(f\"⚠️ No products found for {category}\")\n",
        "        return\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # 4️⃣ Duplicate prevention (check existing product URLs)\n",
        "    # ------------------------------------------------------------\n",
        "    existing_urls = {doc[\"productUrl\"] for doc in collection.find({}, {\"productUrl\": 1})}\n",
        "    new_products = [p for p in all_products if p[\"productUrl\"] not in existing_urls]\n",
        "\n",
        "    if not new_products:\n",
        "        print(f\"⚠️ No new products to insert for {category}\")\n",
        "        return\n",
        "\n",
        "    # Limit to avoid exceeding free tier\n",
        "    new_products = new_products[:limit_per_category]\n",
        "\n",
        "    # Insert into MongoDB\n",
        "    collection.insert_many(new_products)\n",
        "    print(f\"📦 Inserted {len(new_products)} {category} products into MongoDB.\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5️⃣ Run scraper for all categories\n",
        "# ------------------------------------------------------------\n",
        "total_inserted = 0\n",
        "for category, url in categories.items():\n",
        "    scrape_category(category, url)\n",
        "    total_inserted += 1\n",
        "\n",
        "print(\"\\n✅ All categories scraped and inserted successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XYhfmfoFn3Z",
        "outputId": "860343e9-73f9-466c-ad21-e4d56003d062"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔍 Scraping category: smartphones\n",
            "✅ Page 1: 40 items scraped.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2215082809.py:51: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"scrapedAt\": datetime.utcnow()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Page 2: 40 items scraped.\n",
            "✅ Page 3: 40 items scraped.\n",
            "📦 Inserted 50 smartphones products into MongoDB.\n",
            "\n",
            "🔍 Scraping category: laptops\n",
            "✅ Page 1: 40 items scraped.\n",
            "✅ Page 2: 40 items scraped.\n",
            "✅ Page 3: 40 items scraped.\n",
            "⚠️ No new products to insert for laptops\n",
            "\n",
            "🔍 Scraping category: tablets\n",
            "✅ Page 1: 40 items scraped.\n",
            "✅ Page 2: 40 items scraped.\n",
            "✅ Page 3: 40 items scraped.\n",
            "⚠️ No new products to insert for tablets\n",
            "\n",
            "🔍 Scraping category: smartwatches\n",
            "✅ Page 1: 40 items scraped.\n",
            "✅ Page 2: 40 items scraped.\n",
            "✅ Page 3: 40 items scraped.\n",
            "⚠️ No new products to insert for smartwatches\n",
            "\n",
            "✅ All categories scraped and inserted successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 📦 DARAZ Multi-Category Scraper (Skip Smartphones)\n",
        "# ============================================================\n",
        "\n",
        "import requests\n",
        "from pymongo import MongoClient\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1️⃣ MongoDB Connection\n",
        "# ------------------------------------------------------------\n",
        "client = MongoClient(\"mongodb+srv://targetlock_user:momna123@cluster0.n8autuj.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\")\n",
        "db = client[\"test\"]                # ✅ Your main DB (connected to frontend)\n",
        "collection = db[\"products\"]\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2️⃣ Categories to scrape\n",
        "# ------------------------------------------------------------\n",
        "categories = {\n",
        "    # \"smartphones\": \"https://www.daraz.pk/catalog/?q=smartphones\",  # ⛔ SKIPPED intentionally\n",
        "    \"laptops\": \"https://www.daraz.pk/catalog/?q=laptops\",\n",
        "    \"tablets\": \"https://www.daraz.pk/catalog/?q=tablets\",\n",
        "    \"smartwatches\": \"https://www.daraz.pk/catalog/?q=smartwatches\"\n",
        "}\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3️⃣ Function to fetch and insert data\n",
        "# ------------------------------------------------------------\n",
        "def scrape_category(category, base_url, pages=3, limit_per_category=50):\n",
        "    all_products = []\n",
        "    print(f\"\\n🔍 Scraping category: {category}\")\n",
        "\n",
        "    for page in range(1, pages + 1):\n",
        "        url = f\"{base_url}&page={page}&ajax=true\"\n",
        "        r = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "        if r.status_code == 200:\n",
        "            data = r.json()\n",
        "            items = data.get(\"mods\", {}).get(\"listItems\", [])\n",
        "            print(f\"✅ Page {page}: {len(items)} items scraped.\")\n",
        "            for item in items:\n",
        "                product = {\n",
        "                    \"name\": item.get(\"name\"),\n",
        "                    \"price\": item.get(\"price\"),\n",
        "                    \"image\": item.get(\"image\"),\n",
        "                    \"productUrl\": item.get(\"productUrl\"),\n",
        "                    \"ratingScore\": item.get(\"ratingScore\"),\n",
        "                    \"review\": item.get(\"review\"),\n",
        "                    \"sellerName\": item.get(\"sellerName\"),\n",
        "                    \"source\": \"daraz.pk\",\n",
        "                    \"category\": category,\n",
        "                    \"scrapedAt\": datetime.now(timezone.utc)\n",
        "                }\n",
        "                all_products.append(product)\n",
        "\n",
        "    if not all_products:\n",
        "        print(f\"⚠️ No products found for {category}\")\n",
        "        return\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # 4️⃣ Duplicate prevention — per category\n",
        "    # ------------------------------------------------------------\n",
        "    existing_urls = {\n",
        "        doc[\"productUrl\"]\n",
        "        for doc in collection.find({\"category\": category}, {\"productUrl\": 1})\n",
        "    }\n",
        "    new_products = [p for p in all_products if p[\"productUrl\"] not in existing_urls]\n",
        "\n",
        "    if not new_products:\n",
        "        print(f\"⚠️ No new products to insert for {category}\")\n",
        "        return\n",
        "\n",
        "    # Limit per category\n",
        "    new_products = new_products[:limit_per_category]\n",
        "\n",
        "    # Insert into MongoDB\n",
        "    collection.insert_many(new_products)\n",
        "    print(f\"📦 Inserted {len(new_products)} {category} products into MongoDB.\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5️⃣ Run scraper for all categories (smartphones skipped)\n",
        "# ------------------------------------------------------------\n",
        "for category, url in categories.items():\n",
        "    scrape_category(category, url)\n",
        "\n",
        "print(\"\\n✅ All selected categories scraped and inserted successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2LQlJK_GX3N",
        "outputId": "6270d815-da6c-4488-f56b-87c719a174e8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔍 Scraping category: laptops\n",
            "✅ Page 1: 40 items scraped.\n",
            "✅ Page 2: 40 items scraped.\n",
            "✅ Page 3: 40 items scraped.\n",
            "📦 Inserted 50 laptops products into MongoDB.\n",
            "\n",
            "🔍 Scraping category: tablets\n",
            "✅ Page 1: 40 items scraped.\n",
            "✅ Page 2: 40 items scraped.\n",
            "✅ Page 3: 40 items scraped.\n",
            "📦 Inserted 50 tablets products into MongoDB.\n",
            "\n",
            "🔍 Scraping category: smartwatches\n",
            "✅ Page 1: 40 items scraped.\n",
            "✅ Page 2: 40 items scraped.\n",
            "✅ Page 3: 40 items scraped.\n",
            "📦 Inserted 50 smartwatches products into MongoDB.\n",
            "\n",
            "✅ All selected categories scraped and inserted successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i0P_T-VDHOjo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}